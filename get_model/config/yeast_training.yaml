# 基本配置
defaults:
  - _self_
  - model: yeast_model
  - dataset: yeast_dataset
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

stage: "fit"
type: "region"
assembly: "hg38"
eval_tss: false
log_image: false

# Inherit from a finetune config template if available and appropriate
# - finetune: v1_finetune@finetune # Example of inheriting a finetune config

# 实验配置
experiment:
  name: yeast_lora_finetuning
  seed: 42
  debug: false  # 测试模式

# 训练配置
training:
  batch_size: 2
  num_workers: 0
  learning_rate: 0.0001
  weight_decay: 0.0001
  max_epochs: 100
  early_stopping_patience: null
  gradient_clip_val: 1.0
  use_lora: true
  lora_rank: 8
  lora_alpha: 16
  lora_layers: ['encoder', 'head_exp']
  save_every_n_epochs: 1
  accumulate_grad_batches: 2
  clip_grad: 1.0
  use_fp16: true
  log_every_n_steps: 1
  val_check_interval: 1.0
  warmup_epochs: 2

# 数据配置
data:
  data_path: "/root/autodl-tmp/get_model/input/yeast_3d.zarr"
  quantitative_atac: true  # 启用ATAC数据
  # 添加YeastZarrDataset/RegionDataModule所需的参数
  # 例如，如果YeastZarrDataset中实现了数据分割，则添加相关参数

# Feature encoding configuration (May need to align with original GET expectations)
# The 'region_motif' in zarr should already contain encoded features
# feature_encoding:
#   ... (keep or remove as needed based on GETRegionFinetune input)

# 模型配置
model:
  _target_: get_model.model.yeast_model.YeastModel
  cfg:
    num_regions: 3376  # 更新为实际peak数
    num_motif: 284  # motif特征数
    num_conditions: 45  # 实验条件特征数
    embed_dim: 256
    num_layers: 3
    num_heads: 4
    dropout: 0.1
    output_dim: 1
    flash_attn: false
    pool_method: "mean"
    region_embed:
      num_regions: ${model.cfg.num_regions}
      num_features: ${model.cfg.num_motif}
      embed_dim: ${model.cfg.embed_dim}
    encoder:
      num_heads: ${model.cfg.num_heads}
      embed_dim: ${model.cfg.embed_dim}
      num_layers: ${model.cfg.num_layers}
      drop_path_rate: ${model.cfg.dropout}
      drop_rate: 0
      attn_drop_rate: 0
      use_mean_pooling: false
      flash_attn: ${model.cfg.flash_attn}
    head_exp:
      embed_dim: ${model.cfg.embed_dim}
      output_dim: ${model.cfg.output_dim}
      use_atac: false
    loss:
      components:
        exp:
          _target_: torch.nn.MSELoss
          reduction: "mean"
      weights:
        exp: 1.0
    metrics:
      components:
        exp: ["pearson", "spearman", "r2"]
  model:
    name: "yeast_model"
    type: "transformer"
    feature_encoders:
      motif_features:
        type: "linear"
        in_features: 284
        out_features: 32
      media_type:
        type: "embedding"
        num_embeddings: 2
        embedding_dim: 32
        padding_idx: null
      temperature:
        type: "linear"
        in_features: 1
        out_features: 32
      pre_culture:
        time:
          type: "linear"
          in_features: 1
          out_features: 32
        od600:
          type: "linear"
          in_features: 1
          out_features: 32
      drug_culture:
        time:
          type: "linear"
          in_features: 1
          out_features: 32
        drug_name:
          type: "embedding"
          num_embeddings: 100
          embedding_dim: 32
          padding_idx: 0
        concentration:
          type: "linear"
          in_features: 1
          out_features: 32
      carbon_source:
        type: "embedding"
        num_embeddings: 10
        embedding_dim: 32
        padding_idx: 0
      nitrogen_source:
        type: "embedding"
        num_embeddings: 10
        embedding_dim: 32
        padding_idx: 0
    feature_fusion:
      type: "concat"
      fusion_dim: 320  # 10个特征编码器，每个32维
      hidden_dim: 128
      num_layers: 2
      dropout: 0.1
    transformer:
      num_layers: 3
      num_heads: 4
      hidden_dim: 128
      dropout: 0.1
      activation: "gelu"
      layer_norm_eps: 1e-5
    prediction_head:
      type: "mlp"
      hidden_dims: [128, 64]
      dropout: 0.1
      activation: "relu"
      output_dim: 1

# 微调配置
finetune:
  pretrain_checkpoint: true
  checkpoint: 'checkpoints/fetal_pretrain_fetal_finetune/Astrocytes/checkpoint-best.pth'
  strict: false
  model_key: 'state_dict'
  use_lora: true
  lora_checkpoint: null
  layers_with_lora: ['encoder', 'head_exp']
  patterns_to_freeze: []
  patterns_to_drop: []
  additional_checkpoints: []

# 日志配置
logging:
  save_dir: 'output/yeast_finetune'
  tensorboard: true
  wandb: true
  project_name: 'yeast_finetune'
  run_name: 'lora_finetune_v1'
  use_wandb: false

# 机器配置
machine:
  output_dir: "output"
  num_devices: 1  # CPU训练

# 优化器配置
optimizer:
  lr: 0.0001
  min_lr: 1e-6
  weight_decay: 0.05
  opt: 'adamw'
  opt_eps: 1e-8
  opt_betas: [0.9, 0.999]

# 学习率调度器配置
scheduler:
  type: 'cosine'
  T_max: 50  # 与max_epochs一致
  eta_min: 1e-6
  warmup_epochs: 2

# 任务配置
task:
  test_mode: "normal"
  interpret: false
  save_predictions: false
  save_embeddings: false 